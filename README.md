# KoBERT-KorQuAD

- Korean MRC (KorQuAD) with KoBERT
- ğŸ¤—Huggingface TranformersğŸ¤— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ êµ¬í˜„

## Dependencies

- torch >= 1.1.0
- transformers >= 2.4.1
- tensorboardX >= 2.0

## How to use KoBERT on Huggingface Transformers Library

- ê¸°ì¡´ì˜ KoBERTë¥¼ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§ì·„ìŠµë‹ˆë‹¤.
  - transformers v2.2.2ë¶€í„° ê°œì¸ì´ ë§Œë“  ëª¨ë¸ì„ transformersë¥¼ í†µí•´ ì§ì ‘ ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- Tokenizerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `tokenization_kobert.py`ì—ì„œ `KoBertTokenizer`ë¥¼ ì„í¬íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.

```python
from transformers import BertModel
from tokenization_kobert import KoBertTokenizer

model = BertModel.from_pretrained('monologg/kobert')
tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')
```

## Usage

### 1. Training

```bash
$ python3 run_squad.py --model_type kobert \
                       --model_name_or_path monologg/kobert \
                       --output_dir models \
                       --data_dir data \
                       --train_file KorQuAD_v1.0_train.json \
                       --predict_file KorQuAD_v1.0_dev.json \
                       --evaluate_during_training \
                       --per_gpu_train_batch_size 16 \
                       --per_gpu_eval_batch_size 16 \
                       --max_seq_length 512 \
                       --logging_steps 4000 \
                       --save_steps 4000 \
                       --do_train \
                       --do_eval
```

DistilKoBertëŠ” argumentë¥¼ ì•„ë˜ì™€ ê°™ì´ ë°”ê¿”ì£¼ë©´ ë©ë‹ˆë‹¤

- --model_type distilkobert
- --model_name_or_path monologg/distilkobert

### 2. Evaluation

```console
$ python3 evaluate_v1_0.py {$data_dir}/KorQuAD_v1.0_dev.json {$output_dir}/predictions_.json
```

## Results

- Dev setì—ì„œ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼
- max_seq_length = 512
- DistilBert, DistilKoBERTì˜ ê²½ìš° fine-tuningí•  ë•Œ ì¶”ê°€ì ìœ¼ë¡œ Distilationí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤

|                         | Exact Match (%) | F1 Score (%) |
| ----------------------- | --------------- | ------------ |
| KoBERT                  | 52.81           | 80.27        |
| DistilKoBERT            | 54.12           | 77.80        |
| Bert-multilingual       | 70.42           | 90.25        |
| DistilBert-multilingual | 64.32           | 84.78        |

## References

- [KoBERT](https://github.com/SKTBrain/KoBERT)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [KorQuAD](https://korquad.github.io/category/1.0_KOR.html)
- [KorQuAD by graykode](https://github.com/graykode/KorQuAD-beginner)
- [KorQuAD by lyeoni](https://github.com/lyeoni/KorQuAD)
- [KoBert shows low performance on KorQuad](https://github.com/SKTBrain/KoBERT/issues/1)
